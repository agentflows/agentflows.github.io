<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="AgentFlow: In-The-Flow Agentic System Optimization for Effective Planning and Tool Use">
  <meta name="keywords" content="AgentFlow, LLM Agents, Agentic Reasoning, Agentic Systems, RL, Planning, Tool Use">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AgentFlow: In-The-Flow Agentic System Optimization for Effective Planning and Tool Use</title>

  <link rel="icon" href="./assets/logos/agentflow.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/tool_cards.css">
  <link rel="stylesheet" href="./static/css/top_button.css">
  <link rel="stylesheet" href="./static/css/navbar.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/video.css">
  <link rel="stylesheet" href="./static/css/case_visualization.css">
  <link rel="stylesheet" href="./static/css/tool_cards_simplified.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>

  <!-- Add navbar.js script -->
  <script src="./static/js/navbar.js"></script>

  <!-- Add MathJax -->
  <script src="./static/js/mathjax.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- Add visualization.js script -->
  <script src="./static/js/visualization.js"></script>

  <!-- Add case visualization script -->
  <script src="./static/js/case_visualization.js"></script>

  <!-- Add tool cards scripts -->
  <script src="./static/js/tool_cards_data.js"></script>
  <script src="./static/js/tool_cards_simple.js"></script>

  <!-- Add title animation script -->
  <script src="./static/js/title_animation.js"></script>

</head>


<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://zhuofeng-li.github.io/" target="_blank" title="Zhuofeng Li">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>
        <a class="navbar-item" href="https://isaacghx.github.io/about/" target="_blank" title="Haoxiang Zhang">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>
        <a class="navbar-item" href="https://lupantech.github.io/" target="_blank" title="Pan Lu">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://chameleon-llm.github.io/" target="_blank">
              <b>Chameleon</b>
              <p style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</p>
            </a>
            <a class="navbar-item" href="https://textgrad.com/" target="_blank">
              <b>TextGrad</b>
              <p style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</p>
            </a>
            <a class="navbar-item" href="https://mathvista.github.io/" target="_blank">
              <b>MathVista</b>
              <p style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</p>
            </a>
            <a class="navbar-item" href="https://scienceqa.github.io/" target="_blank">
              <b>ScienceQA</b>
              <p style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</p>
            </a>
            <a class="navbar-item" href="https://arxiv.org/abs/2501.06590" target="_blank">
              <b>ChemAgent</b>
              <p style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</p>
            </a>
            <a class="navbar-item" href="https://www.nature.com/articles/s41586-024-07618-3" target="_blank">
              <b>PathChat</b>
              <p style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</p>
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>

  <!-- Burger Menu for Table of Contents -->
  <button class="toc-burger" id="toc-burger" aria-label="Toggle table of contents">
    <span></span>
    <span></span>
    <span></span>
  </button>

  <div class="toc-wrapper" id="toc-wrapper">
    <div class="toc-header">
      <h4>Table of Contents</h4>
      <button class="toc-close" id="toc-close" aria-label="Close table of contents">Ã—</button>
    </div>
    <ul class="toc-list">
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#agentflow">AgentFlow System</a></li>
      <li><a href="#flow-grpo">Flow-GRPO</a></li>
      <li><a href="#tools">Tools</a></li>
      <li><a href="#visualization">Case Studies</a></li>
      <li><a href="#results">Results</a></li>
      <li><a href="#Share">Share</a></li>
      <li><a href="#BibTeX">BibTeX</a></li>
    </ul>
  </div>

  <div class="toc-overlay" id="toc-overlay"></div>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title is-bold">
              <img src="assets/logos/agentflow.png" style="width:1em; vertical-align: middle;" alt="Logo">
              <span style="vertical-align: middle; position: relative; display: inline-block;">
                <span id="agent-text" class="clickable-title"
                      style="color: #000; position: relative; cursor: pointer;">Agent</span>
                <span id="flow-text"
                      style="color: #000; position: relative;">Flow</span>
              </span>
            </h1>
            <h2 class="subtitle is-3 publication-subtitle">
              In-The-Flow Agentic System Optimization for <br>Effective Planning and Tool Use
            </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://zhuofeng-li.github.io/" target="_blank">Zhuofeng Li</a>*<sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="https://isaacghx.github.io/about/" target="_blank">Haoxiang Zhang</a>*<sup>1,3</sup>,</span>
              <span class="author-block">
                <a href="https://seungjuhan.me/" target="_blank">Seungju Han</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://shengliu66.github.io/" target="_blank">Sheng Liu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="http://www.stat.ucla.edu/~jxie/" target="_blank">Jianwen Xie</a><sup>4</sup>,</span><br>
              <span class="author-block">
                <a href="https://yuzhimanhua.github.io/" target="_blank">Yu Zhang</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://yejinc.github.io/" target="_blank">Yejin Choi</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.james-zou.com/" target="_blank">James Zou</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://lupantech.github.io/" target="_blank">Pan Lu</a><sup>1</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> Stanford University</span>
              <span class="author-block"><sup>2</sup> Texas A&M University</span>
              <span class="author-block"><sup>3</sup> UC San Diego</span>
              <span class="author-block"><sup>4</sup> Lambda</span><br>
              <span class="paper-block"><b>* Equal Contribution</b></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2502.11271.pdf"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
                <!-- arxiv link -->
                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/lupantech/AgentFlow"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- HuggingFace Model Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/agentflow"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <p style="font-size:18px">ðŸ¤—</p>
                    </span>
                    <span>Model</span>
                  </a>
                </span>
                <!-- HuggingFace Demo Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/agentflow"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <p style="font-size:18px">ðŸ¤—</p>
                    </span>
                    <span>Demo</span>
                  </a>
                </span>
                <!-- Visualization Link. -->
                <span class="link-block">
                  <a href="#visualization"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <p style="font-size:18px">ðŸ”®</p>
                    </span>
                    <span>Visualize</span>
                  </a>
                </span>
                <!-- Slack Link. -->
                <span class="link-block">
                  <a href="https://agentflowco.slack.com/ssb/redirect"
                    class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <p style="font-size:18px">ðŸ’¬</p>
                    </span>
                    <span>Slack</span>
                  </a>
                </span>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Performance Teaser -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="content has-text-centered">
        <img src="assets/results/fig1_teaser-1.png" width="100%">
        <p style="text-align: left;">
          Performance comparison across 10 diverse benchmarks. <b>AgentFlow</b> with a 7B-scale backbone achieves substantial
          improvements over top-performing baselines across search, agentic, mathematical, and scientific reasoning tasks.
        </p>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="content has-text-centered">
        <img src="assets/models/framework_overall.png" width="100%">
        <p style="text-align: left;">
          <b>(a)</b> Overview of <b>AgentFlow</b>, a trainable agentic system for in-the-flow planning and tool use.
          Four modules (<b>planner</b>, <b>executor</b>, <b>verifier</b>, <b>generator</b>) coordinate via a shared evolving memory $M$ and toolset $K$, given a query $q$.
          <b>The planner policy is optimized on-policy <i>inside</i> the system's multi-turn loop to enable adaptive, long-horizon reasoning.</b>
          <b>(b)</b> A single state transition, showing the action $a^t$, execution result $e^t$, and verifier signal $v^t$ that update
          the memory from $M^t$ to $M^{t+1}$.
        </p>
      </div>
    </div> -->
  </section>


<!-- YouTube Video -->
<!-- <section class="section video-section" id="video">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 video-title">YouTube Video</h2>
        <div class="publication-video video-container">
          <iframe src="https://www.youtube.com/embed/4828sGfx7dk" 
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                  allowfullscreen>
          </iframe>
        </div>
        <p class="has-text-centered mt-4">
          Thanks to <a href="https://www.youtube.com/@code4AI" target="_blank">Discover AI</a> for featuring <span style="color: var(--color-octo-red);"><b>Octo</b></span><span
          style="color: var(--color-tools-blue);"><b>Tools</b></span>!
        </p>
      </div>
    </div>
  </div>
</section> -->

  <!-- Introduction -->
  <section class="section" id="introduction">
    <div class="container" style="margin-bottom: 2vh;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. 
            </p>
            <p>
              We introduce <b>AgentFlow</b>, a trainable, <i>in-the-flow</i> agentic
              framework that coordinates four modules (<b>planner</b>, <b>executor</b>, <b>verifier</b>, <b>generator</b>)
              through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy
              in live environments, we propose <i>Flow-based Group Refined Policy Optimization</i> (<b>Flow-GRPO</b>), which
              tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of
              tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn
              to align local planner decisions with global success and stabilizes learning with group-normalized advantages.
            </p>
            <p>
              Across ten benchmarks, <b>AgentFlow</b> with a 7B-scale backbone outperforms top-performing
              baselines with average accuracy gains of <b>14.9%</b> on search, <b>14.0%</b> on agentic, <b>14.5%</b> on
              mathematical, and <b>4.1%</b> on scientific tasks, even surpassing larger proprietary models like <b>GPT-4o</b>.
              Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced
              tool-calling reliability, and positive scaling with model size and reasoning turns.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!--/ Introduction -->

  <!-- AgentFlow System -->
  <section class="section" id="agentflow">
    <div class="container" style="margin-bottom: 2vh;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"><span <b>AgentFlow</b>: An In-the-Flow Agentic System</h2>

          <!-- AgentFlow Framework Figure -->
          <div class="content has-text-centered" style="margin-bottom: 2rem;">
            <img src="assets/models/framework_overall.png" width="100%">
            <p style="text-align: left;">
              <b>(a)</b> Overview of <b>AgentFlow</b>, a trainable agentic system for in-the-flow planning and tool use.
              Four modules (<b>planner</b>, <b>executor</b>, <b>verifier</b>, <b>generator</b>) coordinate via a shared evolving memory $M$ and toolset $K$, given a query $q$.
              <b>The planner policy is optimized on-policy <i>inside</i> the system's multi-turn loop to enable adaptive, long-horizon reasoning.</b>
              <b>(b)</b> A single state transition, showing the action $a^t$, execution result $e^t$, and verifier signal $v^t$ that update
              the memory from $M^t$ to $M^{t+1}$.
            </p>
          </div>

          <div class="content has-text-justified">
            <p>
              We propose <b>AgentFlow</b>, a general-purpose tool-integrated agentic
              framework for solving complex reasoning tasks through fine-grained planning and effective tool use within a
              multi-turn architecture. As shown in the figure above, the framework comprises four specialized modulesâ€”<b>Action
              Planner</b> $\mathcal{P}$, <b>Tool Executor</b> $\mathcal{E}$, <b>Execution Verifier</b> $\mathcal{V}$, and
              <b>Solution Generator</b> $\mathcal{G}$â€”coordinated by a shared evolving memory $M$ and a toolset $K$. These
              modules interact sequentially and iteratively to perform <i>action planning</i>, <i>tool execution</i>,
              <i>context verification</i>, and <i>solution generation</i>, thereby enabling tool-integrated reasoning across
              multiple turns.
            </p>
            <p>
              We formalize <b>AgentFlow</b>'s problem-solving process as a multi-turn Markov
              Decision Process (MDP). Given a query $q$ and a toolset $K$, the system proceeds for a variable number of turns.
              Let $M^t$ denote the memory state before turn $t$ (with $M^1$ initialized from $q$). At turn $t$, the planner
              $\mathcal{P}$ (a trainable policy $\pi_\theta$) formulates a sub-goal, selects an appropriate tool $k \in K$, and
              retrieves relevant context from memory, producing an action: $a^t \sim \pi_\theta(a^t \mid q, K, M^t)$.
            </p>
            <p>
              The executor $\mathcal{E}$ invokes the chosen tool with context, yielding an execution observation
              $e^t \sim \mathcal{E}(e^t \mid a^t, K)$. The verifier $\mathcal{V}$ then evaluates whether $e^t$ is valid and
              whether the accumulated memory is sufficient to solve the query, producing a binary verification signal
              $v^t \sim \mathcal{V}(v^t \mid q, e^t, M^t)$. If $v^t = 0$, the memory is updated deterministically to
              incorporate new evidence: $M^{t+1} = f_{\text{mem}}(M^t, a^t, e^t, v^t)$, where $f_{\text{mem}}(\cdot)$ denotes
              the memory-update function, which records agent-process information in a concise, structured form along with
              contextual details such as time, turn index, and error signals.
            </p>
            <p>
              The process repeats until $v^t = 1$ (termination) or a predefined maximum turn budget is reached. Upon
              termination at turn $T$, the solution generator $\mathcal{G}$ produces the final solution $o$, conditioned on the
              query and the accumulated memory: $o \sim \mathcal{G}(o \mid q, M^T)$.
            </p>
            <p>
              This formulation decomposes multi-turn, tool-integrated reasoning into structured, observable transitions. After
              $T$ turns, the trajectory $\tau = \{(a^t, e^t, v^t)\}_{t=1}^T$ records the history of planning, execution, and
              verification. The joint generative process can be written as
              <div class="math-scrollable">
              $$p_\theta\!\left(\{a^t, e^t, v^t\}_{t=1}^T,\, o \mid q\right)
              = \Bigg[\prod_{t=1}^T \pi_\theta(a^t \mid q, K, M^t)\;
                \mathcal{E}(e^t \mid a^t, K)\;
                \mathcal{V}(v^t \mid q, e^t, M^t)\Bigg]\;
                \mathcal{G}(o \mid q, M^T),$$
              </div>
              where $\{a^t, e^t, v^t\}_{t=1}^T$ are explicit realizations of the latent reasoning chain. Importantly, unlike
              latent thoughts behind trajectories, our memory $M$ is an explicit and deterministic record of the reasoning
              process, ensuring transparency and controllability of multi-turn decisions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!--/ AgentFlow System -->

  <!-- Flow-GRPO -->
  <section class="section" id="flow-grpo">
    <div class="container" style="margin-bottom: 2vh;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Flow-based Group Refined Policy Optimization</h2>

          <!-- Flow-GRPO Figure -->
          <div class="content has-text-centered" style="margin-bottom: 2rem;">
            <img src="assets/models/Flow-GRPO.png" width="100%">
            <p style="text-align: left;">
              Optimization for our proposed agentic system <b>AgentFlow</b>. Given a query $q$, an evolving memory $M$, and a
              toolset $K$, the policy model generates actions that target sub-goals and select tools. It is trained via
              <i>Flow-based Group Refined Policy Optimization</i> (<b>Flow-GRPO</b>), which enables multi-turn reinforcement
              learning and stable optimization under collaborative dynamics.
            </p>
          </div>

          <div class="content has-text-justified">
            <p>
              We target tool-integrated <i>agentic systems</i> operating under <i>long-horizon</i> tasks with <i>sparse</i>
              rewards. In this setting, the <b>Action Planner</b> (the trainable policy of <b>AgentFlow</b>) selects a <i>sequence</i> of interdependent
              actions while the state $(q, K, M^t)$ evolves with tool results and verifier feedback. Conventional
              <i>offline</i> trainingâ€”e.g., supervised fine-tuning or preference fine-tuning on curated tracesâ€”optimizes the
              planner <i>outside</i> the active loop. This decoupling prevents real-time coordination with the executor,
              verifier, and solution generator, induces distribution shift between training and deployment, and provides
              limited guidance about <i>which</i> intermediate decisions truly matter. As a result, planners often adapt
              poorly to multi-turn dynamics; early errors cascade, and post-hoc fixes are brittle.
            </p>
            <h4 class="title is-5">In-the-flow learning</h4>
            <p>
              To address these issues, we optimize the planner <i>in the flow</i> of execution. We roll out the full
              <b>AgentFlow</b> system under the current policy, collect the
              actual trajectory $\tau$ of states, actions, and tool events it induces, and update the policy within the
              agentic system using a verifiable final-outcome signal. This exposes the multi-turn credit-assignment problem
              directly and trains the planner on the exact states it will face at inference. Our objective, Flow-GRPO, is
              designed to stabilize learning under sparse, trajectory-level rewards over multiple turns.
            </p>
            <p>
              As established above, rollouts in <b>AgentFlow</b> define a finite-horizon MDP with a variable
              horizon $T$. At turn $t$, the planner observes the state $(q, K, M^t)$, selects an action $a^t$, the executor
              and verifier return $(e^t, v^t)$, and the memory updates deterministically to $M^{t+1}$.
            </p>
            <h4 class="title is-5">Policy optimization objective</h4>
            <p>
              The planner policy $\pi_\theta$ is trained to maximize the expected return over on-policy rollouts. Let
              $R(\tau)$ be the reward for a complete trajectory $\tau$. The objective is:
              $$\mathcal{J}(\theta)=\mathbb{E}_{\tau\sim\pi_\theta}\!\big[R(\tau)\big], \qquad \theta^\star=\arg\max_\theta \mathcal{J}(\theta),$$
              where a rollout $\tau$ is the sequence of decisions $\{a^t\}_{t=1}^{T}$ generated on-policy by $\pi_\theta$.
            </p>
            <h4 class="title is-5">Final-outcome reward</h4>
            <p>
              Assigning credit to intermediate actions is challenging because each $a^t$ influences the final solution only
              indirectly, and their value may only emerge after several turns (e.g., error or improvement accumulation). To
              avoid brittle local feedback, we adopt a <i>final-outcome-based reward</i>: every action within a rollout
              receives the same global reward signal, based on the correctness of the final solution $o$ with respect to query
              $q$ and ground truth $y^*$:
              $$r = R(a^t) = \bar{R}(o, q, y^*), \quad \forall t = 1,\dots,T,$$
              where $\bar{R}(o, q, y^*) \in \{0, 1\}$ is assigned by an LLM-as-judge rubric for semantic, numeric, and
              option-level equivalence. This propagates a trajectory-level success signal back through the reasoning chain,
              aligning every decision $a^t$ with global correctness.
            </p>
            <h4 class="title is-5">Objective function</h4>
            <p>
              We formalize <b>Flow</b>-based <b>G</b>roup <b>R</b>efined <b>P</b>olicy <b>O</b>ptimization for the planner.
              The goal is to optimize the policy $\pi_\theta$ by maximizing the expected return over a group of parallel
              rollouts. For each query-label pair from training corpus $(q,y^*) \sim \mathcal{D}$, we sample a group of $G$
              on-policy trajectories $\{\tau_i\}_{i=1}^G$ by running the current behavior policy $\pi_{\theta_\text{old}}$
              inside <b>AgentFlow</b>, where $\tau_i = \{a_i^1, ....a_i^{T_i}, o_i\}$.
              Let $s_i^t=(q, K, M_i^t)$ be the state at turn $t$ of rollout $i$, $a_i^t$ the planner's action (a token
              sequence of length $|a_i^t|$), and $o_i$ the final response.
            </p>
            <p>
              This structure is key to addressing the long-horizon credit assignment challenge: by broadcasting a single
              trajectory-level reward to all turns, we effectively decompose the <i>multi-turn RL</i> problem into <i>a set
              of independent, single-turn</i> policy updates. Each update for an action $a_i^t$ is conditioned on the full
              historical context encapsulated in the state $s_i^t$ and receives the same global success signal, simplifying
              optimization. The objective is
              <div class="math-scrollable">
              $$\begin{aligned}
              \mathcal{J}_{\text{Flow-GRPO}}(\theta) &= \mathbb{E}_{(q, y^*) \sim \mathcal{D}, \; \{\tau_i\}_{i=1}^{G} \sim \pi_{\theta_\text{old}}} \\
              & \Bigg[
              \frac{1}{G}\sum_{i=1}^{G}\frac{1}{T_i}\sum_{t=1}^{T_i}\frac{1}{|a_i^t|}\sum_{j=1}^{|a_i^t|}
              \min\!\Big\{ \rho_{i,j}^t A_i^t,\, \mathrm{clip}(\rho_{i,j}^t,\,1-\epsilon,\,1+\epsilon)\,A_i^t \Big\}
              \;-\; \beta\, \mathbb{D}_{\mathrm{KL}}\!\big(\pi_\theta \,\|\, \pi_{\text{ref}}\big)
              \Bigg],
              \end{aligned}$$
              </div>
              where $T_i$ is the (variable) number of turns in rollout $i$, and
              $$\rho_{i,j}^t
              = \frac{\pi_\theta\!\big(a_{i,j}^t \,\big|\, s_i^t, a_{i,1:j-1}^t\big)}
              {\pi_{\theta_\text{old}}\!\big(a_{i,j}^t \,\big|\, s_i^t, a_{i,1:j-1}^t\big)}$$
              is the token-level importance ratio for the $j$-th token of $a_i^t$, $\epsilon>0$ is the PPO clipping
              parameter, and $\beta>0$ controls the KL penalty to a fixed reference policy $\pi_{\text{ref}}$.
            </p>
            <h4 class="title is-5">Group-normalized advantages</h4>
            <p>
              Because the reward is a single trajectory-level signal, the per-turn advantage $A_i^t$ is constant over $t$
              within a rollout $i$. We reduce variance and sharpen credit assignment across the group by using a
              <i>group-normalized</i> advantage:
              $$A_i^t = \frac{\bar{R}(o_i, q, y^*) - \mathrm{mean}\left( \{ \bar{R}(o_k, q, y^*) \}_{k=1}^{G} \right)}{\mathrm{std}\left( \{ \bar{R}(o_k, q, y^*) \}_{k=1}^{G} \right)}.$$
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!--/ Flow-GRPO -->

  <!-- Tool Cards Section -->
  <section class="section tools-section" id="tools">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Available Tools</h2>
          <p class="subtitle" style="margin-bottom: 2rem;">
            AgentFlow leverages a diverse set of specialized tools to accomplish complex reasoning tasks
          </p>

          <!-- Two-column layout: sidebar + content -->
          <div class="tools-layout">
            <!-- Left sidebar with tool buttons -->
            <div class="tools-sidebar" id="tools-sidebar">
              <!-- Tool selector buttons will be generated by JavaScript -->
            </div>

            <!-- Right content area for tool cards -->
            <div class="tools-content">
              <div class="tools-grid" id="tools-grid">
                <!-- Tool cards will be generated by JavaScript -->
              </div>
            </div>
          </div>

        </div>
      </div>
    </div>
  </section>
  <!--/ Tool Cards -->

<!-- Case Study Visualization -->
<section class="section" id="visualization">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Case Study Visualization</h2>

        <!-- Example Selection Buttons -->
        <div class="case-example-buttons buttons is-centered" style="margin-bottom: 2rem; flex-wrap: wrap;">
          <!-- Buttons will be generated by JavaScript -->
        </div>

        <!-- Case Visualization Container -->
        <div id="case-visualization-container">
          <!-- Case content will be rendered here by JavaScript -->
        </div>

      </div>
    </div>
  </div>
</section>

<!--  &lt;!&ndash; Framework &ndash;&gt;-->
<!--  <section class="section" id="framework">-->
<!--    <div class="container" style="margin-bottom: 2vh;">-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--        <div class="column is-four-fifths">-->
<!--          <h2 class="title is-3">The <span style="color: var(&#45;&#45;color-octo-red);"><b>Octo</b></span><span-->
<!--              style="color: var(&#45;&#45;color-tools-blue);"><b>Tools</b></span> Framework</h2>-->
<!--          <div class="content has-text-justified">-->
<!--            <p>-->
<!--              We propose <span style="color: var(&#45;&#45;color-octo-red);"><b>Octo</b></span><span-->
<!--                style="color: var(&#45;&#45;color-tools-blue);"><b>Tools</b></span>, an open-source, versatile, and-->
<!--              user-friendly agent-toolbox framework for <b>complex reasoning</b> tasks. Given a user query $q \in-->
<!--              \mathcal{Q}$ and a pretrained language model $\text{LLM}_\theta(\cdot)$, a naive approach would generate-->
<!--              an output directly as $y \sim \text{LLM}_\theta(q)$, providing a single-step response. In contrast, our-->
<!--              <span style="color: var(&#45;&#45;color-octo-red);"><b>Octo</b></span><span-->
<!--                style="color: var(&#45;&#45;color-tools-blue);"><b>Tools</b></span> framework introduces a <b>structured</b>,-->
<!--              <b>multi-step</b> process that leverages <b>external tools</b> to tackle queries effectively.-->
<!--            </p>-->
<!--            <p>-->
<!--              Specifically, <span style="color: var(&#45;&#45;color-octo-red);"><b>Octo</b></span><span-->
<!--                style="color: var(&#45;&#45;color-tools-blue);"><b>Tools</b></span> contains a set of <b>tools</b> $\mathcal{D}-->
<!--              = \{d_i\}_{i=1}^n$ and associated metadata $\mathcal{M} = \{m_i\}_{i=1}^n$, where $n$ is the number of-->
<!--              available tools. Given a query, a <b>planner</b> (based on a language model) first generates a-->
<!--              <b>tentative plan</b> from a high-level perspective, indicating how these tools can be used to address the-->
<!--              query, which forms the initial <b>context</b> $s_0$. From this plan, the planner determines the initial-->
<!--              <b>action</b> $a_1$ for tool usage, specifying which tool $d_1$ to use, the relevant context, and a-->
<!--              sub-goal. An <b>executor</b> (also powered by a language model) then converts the planner's text-based-->
<!--              action $a_1$ into a machine-executable <b>command</b> $o_t$, which is run to obtain intermediate results-->
<!--              $r_1$. These results, along with the original action, update the context to $s_1 := (a_1, o_1, r_1)$. This-->
<!--              process constitutes one step in our framework.-->
<!--            </p>-->
<!--            <p>-->
<!--              This process repeats, with the planner iteratively refining its actions based on the evolving context-->
<!--              until it either finds a complete solution or inference limits (e.g., time or steps) are reached. After $T$-->
<!--              steps, the framework produces a full <b>trajectory</b> $(s_0, s_1, \dots, s_T)$, which is stored in a-->
<!--              structured manner in the context. The planner then uses this trajectory to generate the <b>final-->
<!--                solution</b> to the original query.-->
<!--            </p>-->
<!--            <p>-->
<!--              To sum up, <span style="color: var(&#45;&#45;color-octo-red);"><b>Octo</b></span><span-->
<!--                style="color: var(&#45;&#45;color-tools-blue);"><b>Tools</b></span> provides a robust and effective framework-->
<!--              for solving complex tasks through sub-goal decomposition and systematical tool usage. Standardized <b>tool-->
<!--                cards</b> encapsulate functionality , the <b>planner</b> orchestrates both high-level and low-level task-->
<!--              planning, and the <b>executor</b> instantiates tool calls for each sub-goal.-->
<!--            </p>-->
<!--          </div>-->
<!--          <h3 class="title is-4">Task-Specific Tool Selection</h3>-->
<!--          <div class="content has-text-justified">-->
<!--            <p>-->
<!--              The <span style="color: var(&#45;&#45;color-octo-red);"><b>Octo</b></span><span-->
<!--                style="color: var(&#45;&#45;color-tools-blue);"><b>Tools</b></span> toolbox contains a diverse set of tools-->
<!--              covering different modalities and skills. By leveraging structured tool metadata and robust planning-->
<!--              capabilities, <span style="color: var(&#45;&#45;color-octo-red);"><b>Octo</b></span><span-->
<!--                style="color: var(&#45;&#45;color-tools-blue);"><b>Tools</b></span> demonstrates strong generality when all-->
<!--              available tools are enabled across different tasks. However, when a small set of validation examples are-->
<!--              available for a task, configuring a <b>task-specific subset of tools</b> can further enhance efficiency-->
<!--              and effectiveness.-->
<!--            </p>-->
<!--            <p>-->
<!--              To this end, we propose an automated algorithm to optimize the toolset configuration for each task. Given-->
<!--              $n$ available tools in the toolbox, the total number of possible subsets is $O(2^n)$, which is-->
<!--              prohibitively large. To make this tractable, we employ a greedy search strategy that reduces the-->
<!--              complexity to $O(n)$. Our approach proceeds in three stages.-->
<!--            </p>-->
<!--            <div class="container is-max-desktop">-->
<!--              <div class="content has-text-centered">-->
<!--                <img src="assets/models/algorithm.png" width="50%">-->
<!--              </div>-->
<!--            </div>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </section>-->
<!--  &lt;!&ndash;/ Framework &ndash;&gt;-->


  <!-- Results -->
  <section class="section" id="results">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experimental Results</h2>
          <h3 class="title is-4">Main Results</h3>
          <div class="content has-text-justified">
            <p>
              To comprehensively evaluate tool-use capabilities of <b>AgentFlow</b>, we conduct experiments on four
              types of reasoning tasks: (1) <b>Knowledge-intensive search</b> including Bamboogle, 2Wiki, HotpotQA, and Musique;
              (2) <b>Agentic reasoning</b> such as GAIA (where we adopt the textual split);
              (3) <b>Logic-dense mathematical reasoning</b> including AIME 2024, AMC 23, and Game Of 24; and
              (4) <b>Scientific reasoning</b> including GPQA and MedQA.
            </p>

            <div id="main-results-carousel" class="carousel results-carousel">
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/main_table_1.png" width="100%">
                  <p><b>Accuracy comparison on search-intensive and agentic tasks.</b> 7B-Base refers to Qwen-2.5-7B-Base and 7B-Inst refers to Qwen-2.5-7B-Instruct. AutoGen and our <b>AgentFlow</b> method are agentic systems, which use Qwen-2.5-7B-Instruct for the LLM-powered agents and tools for fair comparison. We visualize the gains of <b>AgentFlow</b> to each baseline in the Î” columns.
                  <br><br><b>Baselines:</b> We compare against four categories of baselines: (1) <i>Open-source LLMs</i>: Qwen-2.5 (7B, 14B, 32B) and Llama-3.3-70B; (2) <i>Proprietary LLMs</i>: GPT-4o-mini and GPT-4o; (3) <i>Tool-integrated reasoning LLMs</i>: Supervised Fine-Tuning (SFT), Iter-RetGen, Search-R1, ZeroSearch, ReSearch, StepSearch, and VerlTool; (4) <i>Training-free agentic system</i>: AutoGen.
                  </p>
                </div>
              </div>

              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/main_table_2.png" width="100%">
                  <p><b>Accuracy comparison of mathematical and scientific reasoning tasks.</b> We visualize the gains of <b>AgentFlow</b> to each baseline in the Î” columns.
                  <br><br><b>Baselines:</b> We compare against four categories of baselines: (1) <i>Open-source LLMs</i>: Qwen-2.5 (7B, 14B) and Llama-3.3-70B, Llama-3.1-405B; (2) <i>Proprietary LLMs</i>: GPT-4o-mini and GPT-4o; (3) <i>Reasoning LLMs</i>: Supervised Fine-Tuning (SFT), SimpleRL-reason, Open-Reasoner-Zero, General-Reasoner, and Luffy; (4) <i>Tool-integrated reasoning LLMs</i>: TIR and ToRL; (5) <i>Training-free agentic system</i>: AutoGen.
                  </p>
                </div>
              </div>
            </div>
          </div>
          <h3 class="title is-4">In-Depth Analysis</h3>
          <div class="content has-text-justified">
            <p>
              We conduct comprehensive analyses to understand the effectiveness of Flow-GRPO and the behavior of <span
                <b>AgentFlow</b> across various dimensions.
            </p>

            <div id="results-carousel" class="carousel results-carousel">
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/table3.png" width="60%">
                  <p><b>Impact of Planner Training Strategies.</b> Experiments demonstrate that training the planner with the online reinforcement learning method, Flow-GRPO, yields a significant 17.2% performance improvement, whereas traditional offline Supervised Fine-Tuning (SFT) results in a catastrophic 19.0% performance collapse.
                  </p>
                </div>
              </div>

              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/figure3.png" width="75%">
                  <p><b>Optimized and Adaptive Tool Selection.</b> After optimization with Flow-GRPO, the planner learns to select the most appropriate tools for different tasks, such as increasing the use of Google Search for the broad-knowledge 2Wiki task while shifting to the more specialized Wikipedia and Web Search for the domain-specific MedQA task.
                  </p>
                </div>
              </div>

              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/figure4.png" width="75%">
                  <p><b>Enhanced Tool-Calling Reliability.</b> The Flow-GRPO training process enhances tool-calling reliability, as evidenced by a consistent decrease in the tool-calling error rate across all tasks, with a reduction of up to 28.4% on the GAIA task.
                  </p>
                </div>
              </div>

              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/figure5.png" width="90%">
                  <p><b>Superior Training Efficiency and Stability.</b> Analysis of training dynamics reveals that Flow-GRPO not only continuously increases rewards (accuracy) while shortening response length but also achieves more stable and sustained performance growth compared to traditional monolithic methods like ToRL.
                  </p>
                </div>
              </div>

              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/figure6.png" width="60%">
                  <p><b>Consistent Gains Across Model Scales.</b> Flow-GRPO's online fine-tuning method delivers consistent and effective performance gains on <b>AgentFlow</b>, regardless of whether the backbone model scales from 3B to 7B parameters.
                  </p>
                </div>
              </div>

              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/figure7.png" width="75%">
                  <p><b>Performance Scaling with Inference Turns.</b> During the inference phase, increasing the maximum allowed interaction turns from 3 to 10 enables <b>AgentFlow</b> to conduct deeper reasoning, leading to continuous improvements in final performance across all tasks.
                  </p>
                </div>
              </div>

              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="assets/results/figure9.png" width="75%">
                  <p><b>Adaptability to Upgraded Tool Engines.</b> The trained <b>AgentFlow</b> system demonstrates strong adaptability, as its overall performance significantly improves when its internal tool engines are upgraded from Qwen-2.5-7B-Instruct to the more powerful GPT-4o.
                  </p>
                </div>
              </div>


            </div>
          </div>
        </div>
      </div>
  </section>
  <!--/ Results -->

  
  <!-- Share Buttons -->
  <section class="section" id="Share">

  <div class="column has-text-centered">
    <h2 class="title is-3 has-text-centered">Share <span style="color: #000;"><b>Agent</b></span><span
      style="color: #000;"><b>Flow</b></span></h2>
    <!-- Social Media Share Buttons -->
    <div class="social-share-buttons" style="margin-top: 20px;">
      <!-- Twitter Share Button -->
      <a href="https://twitter.com/intent/tweet?text=Introducing%20OctoTools:%20An%20Agentic%20Framework%20with%20Extensible%20Tools%20for%20Complex%20Reasoning%20https://octotools.github.io" 
         class="button is-info is-outlined"
         target="_blank">
        <span class="icon">
          <i class="fab fa-twitter"></i>
        </span>
        <span>Share on X (Twitter)</span>
      </a>
      
      <!-- LinkedIn Share Button -->
      <a href="https://www.linkedin.com/feed/?shareActive=true&shareUrl=https%3A%2F%2Fagentflows.github.io" 
         class="button is-info is-outlined"
         style="margin-left: 10px;"
         target="_blank">
        <span class="icon">
          <i class="fab fa-linkedin"></i>
        </span>
        <span>Share on LinkedIn</span>
      </a>
    </div>
  </div>
  </section>

  <!-- BibTeX -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-3 has-text-centered">BibTeX</h2>
      <pre><code>@article{li2025agentflow,
    author = {Li, Zhuofeng and Zhang, Haoxiang and Han, Seungju and Liu, Sheng and Xie, Jianwen and Zhang, Yu and Choi, Yejin and Zou, James and Lu, Pan1},
    title = {AgentFlow: In-The-Flow Agentic System Optimization for Effective Planning and Tool Use},
    journal = {arXiv preprint arXiv:2510.},
    year = {2025}
}</code></pre>
    </div>
  </section>

  <section>
    <div class="section" id="org-banners">
      <!-- First row: TAMU, UCSD, Lambda -->
      <div class="org-banner-row">
        <a href="https://www.tamu.edu/" target="_blank" rel="external">
          <img class="center-block org-banner" src="assets/logos/tamu.png" alt="Texas A&M University">
        </a>
        <a href="https://ucsd.edu/" target="_blank" rel="external">
          <img class="center-block org-banner" src="assets/logos/ucsd.png" alt="UC San Diego">
        </a>
        <a href="https://lambdalabs.com/" target="_blank" rel="external">
          <img class="center-block org-banner" src="assets/logos/lambda.png" alt="Lambda">
        </a>
      </div>
      <!-- Second row: Stanford logos centered -->
      <div class="org-banner-row org-banner-row-centered">
        <a href="https://www.stanford.edu/" target="_blank" rel="external">
          <img class="center-block org-banner" src="assets/logos/stanford.png" alt="Stanford University">
        </a>
        <a href="https://ai.stanford.edu/" target="_blank" rel="external">
          <img class="center-block org-banner" src="assets/logos/ai_stanford.png" alt="Stanford AI Lab">
        </a>
      </div>
    </div>
  </section>


  <!-- License -->
  <footer class="footer">
    <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a
              href="https://mathvista.github.io/">MathVista</a>, licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
    <!-- </div> -->
  </footer>

</body>

  <!-- Back to top button -->
  <button onclick="topFunction()" id="topButton" title="Go to top">
    <i class="fas fa-arrow-up"></i>
  </button>


  
</html>